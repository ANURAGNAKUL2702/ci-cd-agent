#!/usr/bin/env python3
"""
Continuous Learning CI/CD Agent
Analyzes daily production pipeline errors and automatically improves performance
"""

import os
import json
import yaml
import re
import datetime
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from enterprise_cicd_agent import EnterpriseGradeCICDAgent

# Setup logging for continuous improvement tracking
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('agent_learning.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

@dataclass
class ErrorPattern:
    error_text: str
    fix_suggestion: str
    frequency: int
    first_seen: str
    last_seen: str
    confidence: float
    category: str
    pattern_type: str

@dataclass
class LearningReport:
    date: str
    pipelines_processed: int
    new_patterns_discovered: int
    existing_patterns_improved: int
    success_rate_improvement: float
    top_error_categories: List[str]

class ContinuousLearningAgent:
    """Agent that learns from daily errors and improves automatically"""
    
    def __init__(self, learning_data_path: str = "learning_data"):
        self.learning_data_path = Path(learning_data_path)
        self.learning_data_path.mkdir(exist_ok=True)
        
        self.patterns_file = self.learning_data_path / "learned_patterns.json"
        self.performance_file = self.learning_data_path / "performance_history.json"
        self.daily_errors_file = self.learning_data_path / "daily_errors.json"
        
        self.learned_patterns = self._load_learned_patterns()
        self.performance_history = self._load_performance_history()
        self.agent = EnterpriseGradeCICDAgent()
        
        logging.info("ðŸ§  Continuous Learning Agent initialized")
    
    def _load_learned_patterns(self) -> Dict[str, ErrorPattern]:
        """Load previously learned error patterns"""
        if self.patterns_file.exists():
            with open(self.patterns_file, 'r') as f:
                data = json.load(f)
                return {k: ErrorPattern(**v) for k, v in data.items()}
        return {}
    
    def _save_learned_patterns(self):
        """Save learned patterns to file"""
        data = {k: asdict(v) for k, v in self.learned_patterns.items()}
        with open(self.patterns_file, 'w') as f:
            json.dump(data, f, indent=2)
    
    def _load_performance_history(self) -> List[Dict]:
        """Load performance history"""
        if self.performance_file.exists():
            with open(self.performance_file, 'r') as f:
                return json.load(f)
        return []
    
    def _save_performance_history(self):
        """Save performance history"""
        with open(self.performance_file, 'w') as f:
            json.dump(self.performance_history, f, indent=2)
    
    def analyze_pipeline_error(self, original_content: str, fixed_content: str, 
                              remaining_errors: List[str], file_path: str = "") -> Dict:
        """Analyze what errors were fixed and what remain"""
        
        analysis = {
            'timestamp': datetime.datetime.now().isoformat(),
            'file_path': file_path,
            'original_lines': len(original_content.splitlines()),
            'fixed_lines': len(fixed_content.splitlines()),
            'remaining_errors': remaining_errors,
            'new_patterns_discovered': []
        }
        
        # Analyze remaining errors for pattern extraction
        for error in remaining_errors:
            pattern_key = self._generate_pattern_key(error)
            
            if pattern_key in self.learned_patterns:
                # Update existing pattern
                self.learned_patterns[pattern_key].frequency += 1
                self.learned_patterns[pattern_key].last_seen = datetime.datetime.now().isoformat()
            else:
                # Discover new pattern
                new_pattern = self._extract_error_pattern(error, original_content, fixed_content)
                if new_pattern:
                    self.learned_patterns[pattern_key] = new_pattern
                    analysis['new_patterns_discovered'].append(pattern_key)
                    logging.info(f"ðŸ” New error pattern discovered: {pattern_key}")
        
        return analysis
    
    def _generate_pattern_key(self, error: str) -> str:
        """Generate unique key for error pattern"""
        # Create a normalized pattern key
        normalized = re.sub(r'[^a-zA-Z0-9\-_]', '_', error.lower())
        return normalized[:50]  # Limit length
    
    def _extract_error_pattern(self, error: str, original: str, fixed: str) -> Optional[ErrorPattern]:
        """Extract and categorize error pattern from failed fix"""
        
        # Analyze error context
        category = self._categorize_error(error)
        fix_suggestion = self._suggest_fix_for_error(error, original, fixed)
        
        if not fix_suggestion:
            return None
        
        return ErrorPattern(
            error_text=error,
            fix_suggestion=fix_suggestion,
            frequency=1,
            first_seen=datetime.datetime.now().isoformat(),
            last_seen=datetime.datetime.now().isoformat(),
            confidence=0.8,  # Initial confidence
            category=category,
            pattern_type="auto_discovered"
        )
    
    def _categorize_error(self, error: str) -> str:
        """Categorize error type"""
        error_lower = error.lower()
        
        if any(word in error_lower for word in ['ubuntu', 'windows', 'macos', 'runner']):
            return "runner_specification"
        elif any(word in error_lower for word in ['action', '@v', 'checkout', 'setup']):
            return "action_version"
        elif any(word in error_lower for word in ['env', 'version', 'variable']):
            return "environment_variable"
        elif any(word in error_lower for word in ['yaml', 'syntax', 'mapping', 'indent']):
            return "yaml_structure"
        elif any(word in error_lower for word in ['timeout', 'permission', 'security']):
            return "configuration"
        else:
            return "unknown"
    
    def _suggest_fix_for_error(self, error: str, original: str, fixed: str) -> Optional[str]:
        """Analyze context to suggest fix for unresolved error"""
        
        # Look for similar patterns in original vs fixed content
        error_lines = [line for line in original.splitlines() if error in line]
        
        if not error_lines:
            return None
        
        error_line = error_lines[0].strip()
        
        # Common fix patterns
        fixes = {
            'ubuntu-lat': 'ubuntu-latest',
            'actions/checkout@': 'actions/checkout@v4', 
            'actions/setup-node@': 'actions/setup-node@v4',
            'actions/setup-python@': 'actions/setup-python@v4',
            'NODE_VERSIO': 'NODE_VERSION',
            'PYTHON_VERSIO': 'PYTHON_VERSION',
            'REGISTR': 'REGISTRY',
            'IMAGE_NAM': 'IMAGE_NAME',
            'TERRAFORM_VERSIO': 'TERRAFORM_VERSION',
            'KUBECTL_VERSIO': 'KUBECTL_VERSION',
            'HELM_VERSIO': 'HELM_VERSION'
        }
        
        for broken, correct in fixes.items():
            if broken in error:
                return f"Replace '{broken}' with '{correct}'"
        
        return f"Manual review required for: {error}"
    
    def process_daily_pipelines(self, pipeline_directory: str) -> LearningReport:
        """Process all pipelines from daily production work"""
        
        pipeline_dir = Path(pipeline_directory)
        today = datetime.date.today().isoformat()
        
        pipelines_processed = 0
        new_patterns = 0
        improved_patterns = 0
        total_success_rate = 0
        
        for pipeline_file in pipeline_dir.glob("*.yml"):
            if pipeline_file.name.startswith("broken_") or "test" in pipeline_file.name:
                continue
                
            try:
                with open(pipeline_file, 'r') as f:
                    content = f.read()
                
                # Test current agent performance
                fixed_content = self.agent.fix_production_pipeline(content)
                
                # Validate results
                remaining_errors = self._find_remaining_errors(content, fixed_content)
                success_rate = self._calculate_success_rate(content, remaining_errors)
                
                # Analyze and learn
                analysis = self.analyze_pipeline_error(
                    content, fixed_content, remaining_errors, str(pipeline_file)
                )
                
                pipelines_processed += 1
                new_patterns += len(analysis['new_patterns_discovered'])
                total_success_rate += success_rate
                
                logging.info(f"ðŸ“Š Processed {pipeline_file.name}: {success_rate:.1f}% success rate")
                
            except Exception as e:
                logging.error(f"âŒ Error processing {pipeline_file}: {e}")
        
        # Calculate improvements
        avg_success_rate = total_success_rate / pipelines_processed if pipelines_processed > 0 else 0
        
        # Update performance history
        previous_rate = self.performance_history[-1]['avg_success_rate'] if self.performance_history else 0
        improvement = avg_success_rate - previous_rate
        
        self.performance_history.append({
            'date': today,
            'avg_success_rate': avg_success_rate,
            'pipelines_processed': pipelines_processed,
            'new_patterns': new_patterns
        })
        
        # Save learned data
        self._save_learned_patterns()
        self._save_performance_history()
        
        return LearningReport(
            date=today,
            pipelines_processed=pipelines_processed,
            new_patterns_discovered=new_patterns,
            existing_patterns_improved=improved_patterns,
            success_rate_improvement=improvement,
            top_error_categories=self._get_top_error_categories()
        )
    
    def _find_remaining_errors(self, original: str, fixed: str) -> List[str]:
        """Find errors that weren't fixed"""
        
        known_errors = [
            'ubuntu-lat', 'actions/checkout@', 'gitleaks/gitleaks-action@v',
            'aquasecurity/trivy-action@', 'dependency-check/Dependency-Check_Action@',
            'actions/setup-node@', 'actions/setup-python@', 'NODE_VERSIO', 
            'PYTHON_VERSIO', 'REGISTR', 'IMAGE_NAM', 'TERRAFORM_VERSIO',
            'KUBECTL_VERSIO', 'HELM_VERSIO', 'requirement.txt', 'requir.txt',
            'PYTHONPTH', 'matrix.analysis =', 'needs.security-gate.outputs.security-passed =',
            'github.ref =', 'github.event_name =', 'timeout:', 'actions/cache@',
            'actions/setup-java@', 'docker/setup-buildx-action@', 'docker/login-action@',
            'docker/build-push-action@', 'hashicorp/setup-terraform@', 
            'anchore/sbom-action@', 'actions/upload-artifact@', 'permissions: write-all'
        ]
        
        remaining = []
        for error in known_errors:
            if error in fixed:
                remaining.append(error)
        
        # Check for YAML syntax errors
        try:
            yaml.safe_load(fixed)
        except yaml.YAMLError as e:
            remaining.append(f"YAML_SYNTAX_ERROR: {str(e)}")
        
        return remaining
    
    def _calculate_success_rate(self, original: str, remaining_errors: List[str]) -> float:
        """Calculate success rate based on remaining errors"""
        total_known_issues = 26  # Based on our test pipeline
        fixed_issues = total_known_issues - len(remaining_errors)
        return (fixed_issues / total_known_issues) * 100 if total_known_issues > 0 else 0
    
    def _get_top_error_categories(self) -> List[str]:
        """Get most frequent error categories"""
        categories = {}
        for pattern in self.learned_patterns.values():
            categories[pattern.category] = categories.get(pattern.category, 0) + pattern.frequency
        
        return sorted(categories.keys(), key=lambda x: categories[x], reverse=True)[:5]
    
    def generate_enhancement_code(self) -> str:
        """Generate code to enhance the agent based on learned patterns"""
        
        enhancements = []
        
        for pattern in self.learned_patterns.values():
            if pattern.frequency >= 3:  # Pattern seen multiple times
                fix_code = self._generate_fix_code(pattern)
                enhancements.append(fix_code)
        
        return '\n\n'.join(enhancements)
    
    def _generate_fix_code(self, pattern: ErrorPattern) -> str:
        """Generate Python code to fix this pattern"""
        
        if pattern.category == "runner_specification":
            return f"""
    def _fix_runner_{pattern.error_text.replace('-', '_')}(self, content: str) -> str:
        \"\"\"Auto-generated fix for {pattern.error_text}\"\"\"
        if '{pattern.error_text}' in content:
            content = content.replace('{pattern.error_text}', '{pattern.fix_suggestion.split("'")[3]}')
            self.fixes_applied.append("Auto-learned: {pattern.error_text} â†’ {pattern.fix_suggestion.split("'")[3]}")
        return content"""
        
        elif pattern.category == "action_version":
            return f"""
    def _fix_action_{pattern.error_text.replace('/', '_').replace('@', '_')}(self, content: str) -> str:
        \"\"\"Auto-generated fix for {pattern.error_text}\"\"\"
        if '{pattern.error_text}' in content:
            content = content.replace('{pattern.error_text}', '{pattern.fix_suggestion.split("'")[3]}')
            self.fixes_applied.append("Auto-learned: {pattern.error_text} â†’ {pattern.fix_suggestion.split("'")[3]}")
        return content"""
        
        return f"# TODO: Implement fix for {pattern.category}: {pattern.error_text}"
    
    def update_agent_patterns(self) -> bool:
        """Update the main agent with new learned patterns"""
        
        if len(self.learned_patterns) == 0:
            return False
        
        try:
            # Generate enhancement code
            new_code = self.generate_enhancement_code()
            
            # Save as enhancement module
            with open('modules/auto_learned_patterns.py', 'w') as f:
                f.write(f'''"""
Auto-generated pattern fixes based on continuous learning
Generated on: {datetime.datetime.now().isoformat()}
Total patterns learned: {len(self.learned_patterns)}
"""

{new_code}
''')
            
            logging.info(f"ðŸš€ Generated {len(self.learned_patterns)} new pattern fixes")
            return True
            
        except Exception as e:
            logging.error(f"âŒ Failed to update agent patterns: {e}")
            return False
    
    def generate_daily_report(self, report: LearningReport) -> str:
        """Generate daily learning report"""
        
        report_content = f"""
# ðŸ“Š Daily CI/CD Agent Learning Report - {report.date}

## ðŸŽ¯ Performance Summary
- **Pipelines Processed**: {report.pipelines_processed}
- **New Patterns Discovered**: {report.new_patterns_discovered}
- **Success Rate Improvement**: {report.success_rate_improvement:+.1f}%

## ðŸ” Top Error Categories
{chr(10).join(f"- {category}" for category in report.top_error_categories)}

## ðŸ§  Learning Progress
- **Total Patterns Learned**: {len(self.learned_patterns)}
- **High-Confidence Patterns**: {len([p for p in self.learned_patterns.values() if p.confidence > 0.9])}

## ðŸ“ˆ Historical Performance
{self._format_performance_history()}

## ðŸš€ Recommended Actions
{self._generate_improvement_recommendations()}
"""
        
        # Save daily report
        report_file = self.learning_data_path / f"daily_report_{report.date}.md"
        with open(report_file, 'w') as f:
            f.write(report_content)
        
        return report_content
    
    def _format_performance_history(self) -> str:
        """Format performance history for report"""
        if not self.performance_history:
            return "No historical data available"
        
        history = self.performance_history[-5:]  # Last 5 days
        lines = []
        for entry in history:
            lines.append(f"- {entry['date']}: {entry['avg_success_rate']:.1f}% ({entry['pipelines_processed']} pipelines)")
        
        return '\n'.join(lines)
    
    def _generate_improvement_recommendations(self) -> str:
        """Generate improvement recommendations"""
        recommendations = []
        
        # Analyze frequent errors
        frequent_errors = [p for p in self.learned_patterns.values() if p.frequency >= 5]
        if frequent_errors:
            recommendations.append("1. **High-Priority Fixes**: Focus on patterns seen 5+ times")
        
        # Analyze success rate trends
        if len(self.performance_history) >= 2:
            recent_trend = self.performance_history[-1]['avg_success_rate'] - self.performance_history[-2]['avg_success_rate']
            if recent_trend < 0:
                recommendations.append("2. **Performance Decline**: Investigate recent pattern changes")
            else:
                recommendations.append("2. **Performance Improving**: Continue current learning approach")
        
        # Category-specific recommendations
        top_category = self._get_top_error_categories()[0] if self._get_top_error_categories() else None
        if top_category:
            recommendations.append(f"3. **Focus Area**: Prioritize {top_category} pattern improvements")
        
        return '\n'.join(recommendations) if recommendations else "No specific recommendations at this time"


def run_daily_learning_cycle(pipeline_directory: str = "."):
    """Run the daily learning and improvement cycle"""
    
    print("ðŸ§  Starting Daily Learning Cycle")
    print("=" * 50)
    
    # Initialize learning agent
    learning_agent = ContinuousLearningAgent()
    
    # Process daily pipelines
    report = learning_agent.process_daily_pipelines(pipeline_directory)
    
    # Update agent with new patterns
    patterns_updated = learning_agent.update_agent_patterns()
    
    # Generate and save daily report
    report_content = learning_agent.generate_daily_report(report)
    
    print(f"\nðŸ“Š Daily Learning Results:")
    print(f"   ðŸ“ Pipelines processed: {report.pipelines_processed}")
    print(f"   ðŸ” New patterns discovered: {report.new_patterns_discovered}")
    print(f"   ðŸ“ˆ Performance improvement: {report.success_rate_improvement:+.1f}%")
    print(f"   ðŸ§  Total learned patterns: {len(learning_agent.learned_patterns)}")
    print(f"   ðŸš€ Agent updated: {'âœ…' if patterns_updated else 'âŒ'}")
    
    print(f"\nðŸ“„ Report saved: learning_data/daily_report_{report.date}.md")
    
    return report, learning_agent


if __name__ == "__main__":
    # Run daily learning cycle
    report, agent = run_daily_learning_cycle()
    
    print("\nðŸŽ¯ Ready for continuous improvement!")
    print("Run this daily to keep improving your CI/CD agent performance.")